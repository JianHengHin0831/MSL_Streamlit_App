import torch
import torch.nn as nn
import torch.onnx
import numpy as np
from onnxruntime.quantization import quantize_dynamic, QuantType

# ==========================================
# 1. é…ç½® (å¿…é ˆèˆ‡è¨“ç·´æ™‚å®Œå…¨ä¸€è‡´)
# ==========================================
INPUT_SIZE = 226        # 30 frames x (Pose + Left Hand + Right Hand)
HIDDEN_SIZE = 64
NUM_CLASSES = 90        # âš ï¸ æ³¨æ„ï¼šå¦‚æœä½ åªè¨“ç·´äº† 10 å€‹è©ï¼Œé€™è£¡è¦æ”¹æˆ 10
MODEL_PATH = 'baseline_model.pth'
ONNX_OUTPUT_PATH = 'msl_model.onnx'
QUANTIZED_OUTPUT_PATH = 'msl_model_quant.onnx'

# ==========================================
# 2. å®šç¾©æ¨¡å‹æ¶æ§‹ (å¿…é ˆèˆ‡è¨“ç·´ä»£ç¢¼å®Œå…¨ä¸€è‡´)
# ==========================================
class CustomLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(CustomLSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 32)
        self.output_layer = nn.Linear(32, num_classes)

    def forward(self, x):
        # LSTM å±¤
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x, _ = self.lstm3(x)
        
        # å–æœ€å¾Œä¸€å€‹æ™‚é–“é»çš„ç‰¹å¾µ (Last Timestep)
        x = x[:, -1, :] 
        
        # å…¨é€£æ¥å±¤
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = torch.relu(self.fc4(x))
        x = torch.relu(self.fc5(x))
        x = self.output_layer(x)
        return x

# ==========================================
# 3. åŸ·è¡Œè½‰æ›
# ==========================================
def convert():
    print(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {MODEL_PATH} ...")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = CustomLSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES)
    
    # è¼‰å…¥æ¬Šé‡ (å¼·åˆ¶ä½¿ç”¨ CPU è¼‰å…¥ï¼Œé¿å… CUDA å ±éŒ¯)
    try:
        model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))
        model.eval() # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼ï¼Œé€™å°å°å‡º ONNX å¾ˆé‡è¦
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        print("è«‹æª¢æŸ¥ NUM_CLASSES æ˜¯å¦æ­£ç¢ºï¼Œæˆ– .pth æª”æ¡ˆè·¯å¾‘æ˜¯å¦æ­£ç¢ºã€‚")
        return

    # å»ºç«‹ä¸€å€‹è™›æ“¬è¼¸å…¥ (Dummy Input)
    # å½¢ç‹€: (Batch_Size, Sequence_Length, Features) -> (1, 30, 226)
    # PyTorch éœ€è¦è·‘ä¸€æ¬¡æ•¸æ“šæ‰èƒ½çŸ¥é“æ¨¡å‹çš„çµæ§‹
    dummy_input = torch.randn(1, 30, INPUT_SIZE)

    print(f"æ­£åœ¨è½‰æ›ç‚º ONNX: {ONNX_OUTPUT_PATH} ...")
    
    # å°å‡ºæ¨¡å‹
    torch.onnx.export(
        model,                      # ä½ çš„æ¨¡å‹
        dummy_input,                # è™›æ“¬è¼¸å…¥
        ONNX_OUTPUT_PATH,           # è¼¸å‡ºæª”å
        export_params=True,         # æ˜¯å¦å„²å­˜æ¬Šé‡
        opset_version=12,           # ONNX ç‰ˆæœ¬ (11 æˆ– 12 æ¯”è¼ƒç©©å®š)
        do_constant_folding=True,   # å„ªåŒ–å¸¸æ•¸æŠ˜ç–Š
        input_names=['input'],      # è¼¸å…¥å±¤åç¨±
        output_names=['output'],    # è¼¸å‡ºå±¤åç¨±
        dynamic_axes={              # è¨­å®šå‹•æ…‹ç¶­åº¦ (è®“ Batch Size å¯ä»¥è®Šå‹•)
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )

    print(f"ğŸ‰ è½‰æ›å®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: {ONNX_OUTPUT_PATH}")

    # ==========================================
    # 4. é‡åŒ–æ¨¡å‹ (åŠ é€Ÿæ¨ç†)
    # ==========================================
    try:
        print(f"âš¡ æ­£åœ¨é‡åŒ–æ¨¡å‹ -> {QUANTIZED_OUTPUT_PATH} ...")
        quantize_dynamic(ONNX_OUTPUT_PATH, QUANTIZED_OUTPUT_PATH, weight_type=QuantType.QInt8)
        print(f"âœ… é‡åŒ–å®Œæˆï¼æª”æ¡ˆå·²å„²å­˜ç‚º: {QUANTIZED_OUTPUT_PATH}")
    except Exception as e:
        print(f"âš ï¸ é‡åŒ–å¤±æ•—: {e}")

    print("ä½ å¯ä»¥ä½¿ç”¨ https://netron.app/ æŸ¥çœ‹ç”Ÿæˆçš„æ¨¡å‹çµæ§‹ã€‚")

if __name__ == "__main__":
    convert()