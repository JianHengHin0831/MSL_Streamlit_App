import gradio as gr
import cv2
import numpy as np
import mediapipe as mp
import torch
import torch.nn as nn
from collections import Counter

# ==========================================
# 1. é…ç½®èˆ‡æ¨¡å‹ (ä¿æŒä¸è®Š)
# ==========================================
gestures = np.array([
    'abang', 'ada', 'ambil', 'anak_lelaki', 'anak_perempuan', 'apa', 'apa_khabar', 'arah', 
    'assalamualaikum', 'ayah', 'bagaimana', 'bahasa_isyarat', 'baik', 'baik_2', 'baca', 
    'bapa', 'bapa_saudara', 'bas', 'bawa', 'beli', 'beli_2', 'berapa', 'berjalan', 'berlari', 
    'bila', 'bola', 'boleh', 'bomba', 'buang', 'buat', 'curi', 'dapat', 'dari', 'emak', 
    'emak_saudara', 'hari', 'hi', 'hujan', 'jahat', 'jam', 'jangan', 'jumpa', 'kacau', 
    'kakak', 'keluarga', 'kereta', 'kesakitan', 'lelaki', 'lemak', 'lupa', 'main', 'makan', 
    'mana', 'marah', 'mari', 'masa', 'masalah', 'minum', 'mohon', 'nasi', 'nasi_lemak', 
    'panas', 'panas_2', 'pandai', 'pandai_2', 'payung', 'pen', 'pensil', 'perempuan', 
    'pergi', 'pergi_2', 'perlahan', 'perlahan_2', 'pinjam', 'polis', 'pukul', 'ribut', 
    'sampai', 'saudara', 'sejuk', 'sekolah', 'siapa', 'sudah', 'suka', 'tandas', 'tanya', 
    'teh_tarik', 'teksi', 'tidur', 'tolong'
])
gestures = np.sort(gestures)

MODEL_PATH = 'baseline_model.pth' 
INPUT_SIZE = 226
HIDDEN_SIZE = 64
NUM_CLASSES = len(gestures)
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class CustomLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(CustomLSTM, self).__init__()
        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.fc1 = nn.Linear(hidden_size, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 32)
        self.output_layer = nn.Linear(32, num_classes)

    def forward(self, x):
        x, _ = self.lstm1(x)
        x, _ = self.lstm2(x)
        x, _ = self.lstm3(x)
        x = x[:, -1, :] 
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = torch.relu(self.fc4(x))
        x = torch.relu(self.fc5(x))
        x = self.output_layer(x)
        return x

# è¼‰å…¥æ¨¡å‹
model = CustomLSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_CLASSES).to(DEVICE)
try:
    # é›²ç«¯é€šå¸¸æ˜¯ CPUï¼Œæ‰€ä»¥å¼·åˆ¶ map_location='cpu'
    model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))
    model.eval()
    print("Model Loaded!")
except Exception as e:
    print(f"Model Load Error: {e}")

# MediaPipe
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

# ==========================================
# 2. æ ¸å¿ƒè™•ç†é‚è¼¯
# ==========================================
def extract_keypoints(results):
    if results.pose_landmarks:
        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark[:25]]).flatten()
    else:
        pose = np.zeros(25*4)
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    return np.concatenate([pose, lh, rh])

def predict_frame(image, state):
    # state æ˜¯ä¸€å€‹å­—å…¸ï¼Œç”¨ä¾†åœ¨ä¸åŒå¹€ä¹‹é–“å‚³éæ•¸æ“š
    if state is None:
        state = {"sequence": [], "predictions": [], "sentence": "Waiting...", "frame_count": 0}
    
    if image is None:
        return None, state

    # 1. å½±åƒå‰è™•ç†
    # Gradio å‚³å…¥çš„æ˜¯ RGBï¼ŒMediaPipe ä¹Ÿåƒ RGB
    image.flags.writeable = False
    
    # åˆå§‹åŒ– Holistic (æ¯æ¬¡éƒ½é‡æ–°åˆå§‹åŒ–æœƒæ…¢ï¼Œä½†åœ¨å‡½æ•¸å¼ç·¨ç¨‹ä¸­æ¯”è¼ƒå®‰å…¨)
    # ç‚ºäº†æ•ˆèƒ½ï¼ŒHugging Face æœƒè‡ªå‹•å„ªåŒ–
    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
        results = holistic.process(image)
    
    image.flags.writeable = True
    
    # ç•«åœ–
    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    # æ¸›å°‘ç¹ªè£½èº«é«”ä»¥æå‡é€Ÿåº¦
    # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)

    state["frame_count"] += 1
    
    # 2. é æ¸¬é‚è¼¯ (æ¯ 2 å¹€è™•ç†ä¸€æ¬¡)
    if state["frame_count"] % 2 == 0:
        keypoints = extract_keypoints(results)
        state["sequence"].append(keypoints)
        state["sequence"] = state["sequence"][-30:] # ä¿æŒ 30 å¹€

        if len(state["sequence"]) == 30:
            input_tensor = torch.tensor(np.expand_dims(state["sequence"], axis=0), dtype=torch.float32).to(DEVICE)
            with torch.no_grad():
                res = model(input_tensor)
            
            probs = torch.softmax(res, dim=1).cpu().numpy()[0]
            pred_idx = np.argmax(probs)
            max_prob = probs[pred_idx]
            
            state["predictions"].append(pred_idx)
            state["predictions"] = state["predictions"][-10:]
            
            # ç©©å®šåŒ–é‚è¼¯
            if len(state["predictions"]) > 0:
                most_common_id, frequency = Counter(state["predictions"]).most_common(1)[0]
                if frequency >= 8 and max_prob > 0.7:
                    state["sentence"] = gestures[most_common_id]
                    # ç¶ è‰²æ¢æ¢
                    cv2.rectangle(image, (0,0), (int(max_prob*200), 40), (0,255,0), -1)
                elif max_prob > 0.7:
                    # é»ƒè‰²æ¢æ¢
                    cv2.rectangle(image, (0,0), (int(max_prob*200), 40), (0,255,255), -1)

    # 3. ç¹ªè£½æ–‡å­—
    cv2.rectangle(image, (0, 40), (640, 80), (245, 117, 16), -1)
    cv2.putText(image, state["sentence"], (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
    
    # é¡åƒç¿»è½‰å›å‚³
    return cv2.flip(image, 1), state

# ==========================================
# 3. Gradio ä»‹é¢
# ==========================================
with gr.Blocks(title="MSL Recognition AI") as demo:
    gr.Markdown("# ğŸ‡²ğŸ‡¾ Malaysian Sign Language Recognition")
    gr.Markdown("Stand back and show your upper body. Perform signs slowly.")
    
    with gr.Row():
        with gr.Column():
            # sources=["webcam"] é–‹å•Ÿæ”åƒé ­
            # streaming=True é–‹å•Ÿå³æ™‚æµæ¨¡å¼
            input_video = gr.Image(sources=["webcam"], streaming=True, label="Input Camera")
        with gr.Column():
            output_video = gr.Image(label="AI Output")
    
    # ç”¨ä¾†è¨˜æ†¶ç‹€æ…‹çš„è®Šæ•¸
    state = gr.State()
    
    # ç•¶è¼¸å…¥å½±åƒæ”¹è®Šæ™‚ï¼Œå‘¼å« predict_frame
    input_video.stream(
        predict_frame, 
        [input_video, state], 
        [output_video, state]
    )

if __name__ == "__main__":
    demo.launch()